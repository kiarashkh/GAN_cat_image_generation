{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiarashkh/GAN_cat_image_generation/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q6bJB20Q2dKA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\kiara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def make_generator(latent_dim=100):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Step 1: Dense layer to project noise into a feature map\n",
        "    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(latent_dim,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.ReLU())\n",
        "    model.add(layers.Reshape((8, 8, 256)))  # (H, W, C)\n",
        "\n",
        "    # Step 2: Transposed Conv → upsample to 16x16\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    # Step 3: Transposed Conv → upsample to 32x32\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    # Step 4: Transposed Conv → upsample to 64x64 with 3 channels (RGB)\n",
        "    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False, activation=\"tanh\"))\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_discriminator(image_shape=(64, 64, 3)):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Step 1: Convolution block → downsample 64x64 -> 32x32\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\", \n",
        "                            input_shape=image_shape))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.3))  # helps prevent overfitting\n",
        "\n",
        "    # Step 2: Downsample 32x32 -> 16x16\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # Step 3: Downsample 16x16 -> 8x8\n",
        "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # Step 4: Flatten + final dense\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))  # output: logit (no sigmoid if using from_logits=True)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\kiara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\kiara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "generator = make_generator(latent_dim=100)\n",
        "discriminator = make_discriminator()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# Loss functions\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)   # real → 1\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output) # fake → 0\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)  # trick D into thinking fake=real\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Typical DCGAN-style hyperparameters\n",
        "LR = 2e-4          # learning rate (0.0002)\n",
        "BETA_1 = 0.5       # Adam beta1 (important for GAN stability)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=LR, beta_1=BETA_1)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=LR, beta_1=BETA_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(real_images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
        "\n",
        "    # 1) compute both losses & gradients\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        fake_images = generator(noise, training=True)\n",
        "\n",
        "        real_logits = discriminator(real_images, training=True)\n",
        "        fake_logits = discriminator(fake_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_logits)\n",
        "        disc_loss = discriminator_loss(real_logits, fake_logits)\n",
        "\n",
        "    # 2) gradients\n",
        "    gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    # 3) optional: gradient clipping to stabilize\n",
        "    gen_grads, _ = tf.clip_by_global_norm(gen_grads, 5.0)\n",
        "    disc_grads, _ = tf.clip_by_global_norm(disc_grads, 5.0)\n",
        "\n",
        "    # 4) apply gradients with the optimizers we defined\n",
        "    generator_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMG_SIZE = 64   # we want 64x64 for DCGAN\n",
        "\n",
        "def preprocess_image(path):\n",
        "    # Read file from disk\n",
        "    img = tf.io.read_file(path)\n",
        "    # Decode JPEG (or PNG), keep 3 channels (RGB)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    # Resize to target size\n",
        "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
        "    # Scale from [0,255] → [-1,1] (tanh expects this)\n",
        "    img = (img - 127.5) / 127.5\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# List all JPEGs inside ./cats\n",
        "cat_paths = tf.data.Dataset.list_files(\"./cats/*.jpg\", shuffle=True)\n",
        "\n",
        "# Map preprocessing\n",
        "cat_dataset = cat_paths.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Shuffle, batch, prefetch\n",
        "cat_dataset = (cat_dataset\n",
        "               .shuffle(buffer_size=1000)\n",
        "               .batch(BATCH_SIZE, drop_remainder=True)\n",
        "               .prefetch(tf.data.AUTOTUNE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for batch in cat_dataset.take(1):\n",
        "    print(batch.shape)  # (64, 64, 64, 3) → batch of 64 images\n",
        "    plt.imshow((batch[0] + 1) / 2.0)  # convert back from [-1,1] → [0,1]\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def show_real_and_fake(generator, dataset, epoch, latent_dim=100):\n",
        "    # Get one batch of real images\n",
        "    real_batch = next(iter(dataset))\n",
        "    \n",
        "    # Take 2 real images\n",
        "    real_images = real_batch[:2]\n",
        "\n",
        "    # Generate 2 fake images\n",
        "    noise = tf.random.normal([2, latent_dim])\n",
        "    fake_images = generator(noise, training=False)\n",
        "\n",
        "    # Convert from [-1,1] → [0,1] for display\n",
        "    real_images = (real_images + 1) / 2.0\n",
        "    fake_images = (fake_images + 1) / 2.0\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(6, 6))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Show 2 real\n",
        "    for i in range(2):\n",
        "        axes[i].imshow(real_images[i].numpy())\n",
        "        axes[i].set_title(\"Real\")\n",
        "        axes[i].axis(\"off\")\n",
        "\n",
        "    # Show 2 fake\n",
        "    for i in range(2):\n",
        "        axes[i+2].imshow(fake_images[i].numpy())\n",
        "        axes[i+2].set_title(\"Fake\")\n",
        "        axes[i+2].axis(\"off\")\n",
        "\n",
        "    plt.suptitle(f\"Epoch {epoch}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    for real_batch in cat_dataset:\n",
        "        g_loss, d_loss = train_step(real_batch)\n",
        "\n",
        "    print(f\"Epoch {epoch} | Generator loss: {g_loss:.4f}, Discriminator loss: {d_loss:.4f}\")\n",
        "\n",
        "    # At the end of epoch: show 2 real + 2 fake\n",
        "    show_real_and_fake(generator, cat_dataset, epoch, latent_dim=100)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNuxOfpAu7WqsJeBUQ3NcV3",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
